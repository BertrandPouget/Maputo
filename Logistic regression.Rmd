---
title: "Logistic regression"
output:
  html_notebook: default
  word_document: default
---
# Some theory
Regression tries to establish a relationship between a dependent variable, the target, and some independent variables, the predictors. 
We use logistic regression whenever the outcome of the dependent variable is discrete (e.g. 0, 1 or TRUE, FALSE or YES, NO).
The outcome of the logistic model is actually a probability according to which we will obtain either 0 or 1 depending on a threshold. This means that the target variable follows a Binomial distribution:
$$
Y_i \sim Bin(n_i,p_i),\\ p_i={\frac{e^{\beta_0+\beta_1 x_{i1}+...+\beta_p x_{ip}}}{1+e^{\beta_0+\beta_1 x_{i1}+...+\beta_p x_{ip}}}}=\mathbb{P}(Y_i=1|\underline{x}),\\
logit(p_i)=\beta_0+\beta_1x_i1+...+\beta_px_ip
$$
The logistic regression estimates the parameters of the model $\beta_0$,$\beta_1$,$...$,$\beta_p$ using MLE since the goal is to maximize the probability of observing the given data.

Logistic regression is a technique used for classification.

# Logistic regression on the streets of Maputo based on mean values and variance of the colors
## Building the model
We are working on the dataset 'datonite' which contains the information on the streets of the capital (length, typo, geometry, surface, ...) plus 12 more additional covariates: the mean, the variance, the minimum and maximum for the colors red, green and blue.
The goal is to be able to predict if a street is paved or unpaved based on the distribution of the colors.

The first thing we need to do is to set the working directory, install all the library and packages that we need and import the dataset. We aslo need to select only the first 2558 rows of the dataset, since for the remaining ones we do not know the condition of the surface, therefore we are going to use the model to predict it.
```{r}
setwd("C:/Users/user/OneDrive/Documenti/Magistrale/Applied Statistics/Project")

library(sf)
library(rgdal)
library(dplyr)
library(rms)
library(arm)
library(ResourceSelection)
library(pROC)
library(caTools)
library(ROCR)
library(sp)
library(ggplot2)
library(lme4)
library(caret)
library(PRROC)
library(DAAG)
library(GGally)
library(car)
library(tidyverse)

datonite = st_read("datonite.shp")
summary(datonite)
data = datonite[1:2558,]
summary(data)
```
From now on we consider the dataset 'data'.
The target variable is 'osm_surf' which is of type character and it can assume the "values" 'paved' or 'unpaved'.
Using the following line we set 'paved' as 1 and 'unpaved' as 0.
```{r}
data$osm_surf = ifelse(data$osm_surf == "paved",1,0)
View(data)
```

Now we need to split the dataset into two subset: the first, which contains the 80% of the data, is the training set and the second is the test set. We are going to work on the training set to build the model and for its validation, while we are gonna test the accuracy of its prediction on the test set. Only then we will be able to use our model to do some prediction on a new dataset containing the remaining 2558 streets for which we do not know their surface.
```{r}
set.seed(12052022)
split = sample.split(data,SplitRatio = 0.8)
split
training = subset(data,split == "TRUE")
rownames(training) <- 1:nrow(training)
dim(training)
View(training)
testing = subset(data,split == "FALSE")
rownames(testing) <- 1:nrow(testing)
dim(testing)
View(testing)
attach(training)
```
MANOVA
```{r}
data <- data[order(data$osm_surf),]
rownames(data) <- 1:nrow(data)

dat_num <- cbind(data$rmean, data$rvar, data$rmed)
dat_num <- as.data.frame(dat_num)
dat_num <- dat_num %>% rename(rmean = V1, rvar = V2, rmed = V3)

categories <- factor(data$osm_surf, labels=c(1, 0))

i1 <- which(categories==1)
i2 <- which(categories==0)

par(mfrow=c(1,3))
boxplot(dat_num$rmean~categories, col = rainbow(2))
boxplot(dat_num$rvar~categories, col = rainbow(2))
boxplot(dat_num$rmed~categories, col = rainbow(2))
```


Finally, we can build our model using the means and variances of the colors as predictors.
```{r}
mod1 = glm(osm_surf ~rmean+rvar+gmean+gvar+bmean+bvar, 
               family = binomial(link = logit))
summary(mod1)
```
The command 'summary()' reports the estimates of the intercept and the coefficients of all the predictors, with their standard error (which helps to quantify how good an estimate is) and their statistical tests. By looking at the statistical test, one can conclude that the predictors 'rvar' and 'gmean' are not statistical significant and so we can think of creating another model without using these two variables.
```{r}
mod2 = glm(osm_surf ~ rmean+gvar+bmean+bvar, family = binomial(link = logit))
summary(mod2)
```
We are usually interested in the relative goodness of the model, which can be quantifies by the AIC and usually we want it to be lower than possible. 
By comparing the results of the two models we can see that the AIC of the second model is lower than the first one. However, in the second model the residual deviance is higher. The residual deviance tells us how well the response variable can be predicted by a model with p predictor variables. The lower the value, the better the model is able to predict the value of the response variable.

We could also check on the BIC for the two model, knowing that the BIC is a variant of AIC with a stronger penalty for including additional variables to the model.
```{r}
BIC(mod1)
BIC(mod2)
```
And, how we expected, the BIC for the second model is lower than the one for the original model.

So, how can we choose which model is the best? We can perform the ANOVA between the two models.
```{r}
anova(mod1,mod2,test = "Chisq")
```
The 'Chisq' value is the test statistic of the likelihood ratio test (LRT) being applied to the two models. This value is computed as twice the difference in the log-likelihoods of the two models. Asymptotically, the log-likelihood ratio follows a Chi-square distribution with degrees of freedom equal to the difference in degrees of freedom of the two models; here this is 2 (-2) and is shown in the 'Df' column. As such, the probability of observing a test statistic if the two models were equivalent can be computed from that Chi-square distribution. This probability is the p-value (0.287).
In more practical terms, this just reaffirms the interpretation of the relative merits of the two models made via AIC. The probability of the test statistic is large if the two models provided the same fit. Hence we fail to reject the null hypothesis that the likelihoods of the two models are equivalent which means that the model with less predictors is not less informative than the more complex model and they are essentially the same.
'mod.surf2' is "better" in the sense that it does as well as the more complex model with 1 fewer parameters. The AICs of the two models differ by almost 2 AIC units which is, from the definition of AIC, what we would expect if we added a redundant parameter with no additional explanatory power to the model.

An important concept to understand for interpreting the logistic coefficients is the odds ratio. An odds ratio measures the association between a predictor and the outcome and it represents the ratio of odds that an event will occur given the presence of the predictor (the predictor is set to be equal to 1), compared to the odds of the event occurring in the absence of the predictor (the predictor is set to be equal to 0).
```{r}
OR = exp(mod2$coefficients)
OR
```

From now on, we can use the model with less predictors.

## Diagnostic and goodness of fit
To know if the prediction we made with our model are actually good we build the confusion matrix, but first we need to fix a cutoff. When we choose a threshold (initially, we are going to choose 0.4), we are saying that we would like to classify every observation with a predicted probability from the model equal to or greater than 0.4 as a “success” and we will classify observations meeting this criteria as a success regardless if that outcome was actually observed to be a success.
```{r}
real.values = osm_surf
predicted.values = mod2$fitted.values
threshold = 0.5
predicted.values = as.numeric(predicted.values>threshold)
confusion.matrix = table(real.values,predicted.values)
confusion.matrix
```
In the confusion matrix we have reported the observations which are classified as: 

* __True Positive__ when the real value is 1 and it is predicted as 1

* __True Negative__ when the real value is 0 and it is predicted as 0

* __False Positive__ when the real value is 0, but it is predicted as 1

* __False Negative__ when the real value is 1, but it is predicted as 0

We can use the information reported in the confusion matrix to evaluate the performance of our model in terms of the quality of its predictions. For example one can compute:

* the __accuracy__ of the model which is the proportion of correct predictions on the total: 
$$
{\displaystyle \mathrm {Accuracy} ={\frac {\mathrm {TP} +\mathrm {TN} }{\mathrm {TP} +\mathrm {TN} +\mathrm {FP} +\mathrm {FN} }}}
$$

* the __sensitivity__ which is the proportion of true positive classified as positive:
$$
{\displaystyle \mathrm {Sensitivity} ={\frac {\mathrm {TP} }{\mathrm {TP} +\mathrm {FN} }}}
$$

* the __specificity__ which is the proportion of true negative classified as negative:
$$
{\displaystyle \mathrm {Specificity} ={\frac {\mathrm {TN} }{\mathrm {TN} +\mathrm {FP} }}}
$$

```{r}
TP = confusion.matrix[2,2]
TN = confusion.matrix[1,1]
FP = confusion.matrix[1,2]
FN = confusion.matrix[2,1]
#Accuracy
Accuracy = (TP+TN)/(TP+TN+FP+FN)
Accuracy
#Sensitivity
Sensitivity = TP/(TP+FN)
Sensitivity
#Specificity
Specificity = TN/(TN+FP)
Specificity
```
How can we say if the model we have built is good? In medical application the answer to this question can be found by looking at the confusion matrix.
The value of FN is very dangerous: a false negative is when a patient is diagnosed to be fine, but in reality they are positive to the disease and we want the FN value to be minimum which is the same of asking the sensitivity to be maximal.
In our scenario, FN value represents a street that is predicted to be unpaved, but in reality its surface is asphalted. The goal of this analysis is to do classification and this can be used to organize an operation on the streets of the capital, to pave all those streets which are still unpaved. An estimate on the number of unpaved streets is needed to create a budget and, in our opinion, it is better to predict more paved streets than unpaved once, also because once we count the FN to create the budget, it is a waste of resources and money since those streets are already paved.
In conclusion, we need to work on the threshold to maximize the sensitivity and minimize the FN.

How can we control the sensitivity? Since the confusion matrix is build once we have fixed a threshold, one way could be to check how accuracy, sensitivity and specificity change choosing different values for the threshold. A more practical way to find the optimal threshold is using the ROC curve.
```{r}
threshold_roc = seq(0,1,length.out=2e2)
lens = length(threshold_roc)-1
roc_x = rep(NA,lens)
roc_y = rep(NA,lens)
for (k in 1:lens) {
  threshold = threshold_roc[k]
  classification = as.numeric(sapply(mod2$fitted.values, function(x) ifelse(x<threshold,0,1)))
  roc_y[k] = sum(classification[which(osm_surf==1)]==1)/
    length(which(osm_surf==1))
  roc_x[k] = sum(classification[which(osm_surf==0)]==1)/
    length(which(osm_surf==0))
}
plot(roc_x,roc_y,type = "l",xlab = "1 - Specificity",ylab = "Sensitivity",
      main = "Curva ROC",lwd = 2,col = 'darkblue',ylim = c(0,1),xlim=c(0,1))
abline(h = c(0,1),v = c(0,1),lwd = 1,lty = 2,col = 'red')
abline(a = 0,b = 1,lty = 2,col = 'black')
abline(v = 1 - Specificity,h = Sensitivity,lty = 3,col = 'blue')
points(1-Specificity,Sensitivity,pch = 4,lwd = 3,cex = 1.5,col = 'blue')
```
Each dot on the curve represents a different possible cutoff value for classifying predicted values. We could pick any value between 0 and 1 as the threshold, but doing this manually for every possible meaningful cutoff value would be exhausting. So what a ROC curve does is looking at every possible cutoff value that results in a change of classification of any observation in the data set: fo example, if stepping the classification cutoff up from 0.4 to 0.5 does not result in a change in how the observations are classified, then it’s not an interesting step to consider. 

Let's talk about the plot. The Y axis is sensitivity, or true positive rate, while the X axis is 1-specificity which is equivalent to the false positive rate. For every point on the ROC curve (representing a different cutoff value), the location of that point is plotted as the sensitivity at that cutoff value on the Y axis, and 1 – specificity at that cutoff value on the X axis. As such, the ROC curve shows graphically the tradeoff that occurs between trying to maximize the true positive rate vs. trying to minimize the false positive rate. In an ideal situation, we would have sensitivity and specificity near 100% at all cutoffs, meaning you predict perfectly in all cases. 

How do you determine which cutoff to use? It depends on the specific scenario. Even though the package 'pROC' has a function to find the "best" cutoff value, we cannot make a decision like this only based on some information metric. If FN are worse than FP, then choose a threshold with high sensitivity. Alternatively, pick a cutoff with high specificity (values to the left in the ROC graph).
```{r}
roc_obj = roc(real.values,predicted.values)
coords(roc_obj,"best")
```
If we think of the meaning, in our research, of the cutoff value, it is clear that we cannot accept a threshold of 0.5, because it is like saying that we expect to find a paved street with the same probability of finding an unpaved one.

The Area Under the ROC curve (AUC) is an aggregated metric that evaluates how well a logistic regression model classifies positive and negative outcomes at all possible cutoffs. It can range from 0.5 to 1, and the larger it is the better.
```{r}
auc(roc_obj)
```

A better way to find the optimal threshold is by doing a cross-validation and, in particular, we are interested in the k-fold cross-validation which evaluates the model performance on different subset of the training data and then calculate the average prediction error rate. This is a robust method for estimating the accuracy of a model. How to chose k? Lower value of K is more biased and hence undesirable. On the other hand, higher value of K is less biased, but can suffer from large variability. We are gonna set k equal to 10.
```{r}
# Acc = c()
# Sens = c()
# Spec = c()
# RMSE = c()
# threshold = 0.35
# 
# for (k in 1:10) {
#   validation = training[((k-1)*round(0.1*dim(training)[1])+1):(k*round(0.1*dim(training)[1])-1),]
#   validation = na.omit(validation)
#   
#   train = training[-c(((k-1)*round(0.1*dim(training)[1])+1):(k*round(0.1*dim(training)[1])-1)),]
#   train = na.omit(train)
#   
#   mod.surf2 = glm(osm_surf ~ rmean+gvar+bmean+bvar, data = train, family = binomial(link = logit))
#   
#   real.values = validation$osm_surf
#   
#   predicted.values = as.numeric(predict(mod.surf2,newdata=validation,type="response")>threshold)
#   
#   confusion.matrix = table(real.values,predicted.values)
#   confusion.matrix
#   TP = confusion.matrix[2,2]
#   TN = confusion.matrix[1,1]
#   FP = confusion.matrix[1,2]
#   FN = confusion.matrix[2,1]
#   Acc = c(Acc,(TP+TN)/(TP+TN+FP+FN))
#   Sens = c(Sens,TP/(TP+FN))
#   Spec = c(Spec,TN/(TN+FP))
#   RMSE = c(RMSE,sqrt(mean((real.values-predicted.values)^2)))
#   
#   #remove(validation)
#   #remove(train)
# }
# 
# mean(Acc)
# mean(Sens)
# mean(Spec)
# mean(RMSE)
```
The Root Mean Squared Error (RMSE) measures the average prediction error made by the model in predicting the outcome for an observation. That is, the average difference between the observed known outcome values and the values predicted by the model. The lower the RMSE, the better the model.

The final model error is taken as the mean error from the number of repeats.

We repeat the validation with different values for the threshold keeping in mind that our goal is to maximize the sensitivity of the model.
```{r}
threshold = c(0.2,0.25,0.3,0.35,0.4,0.45,0.5)
accuracy = c()
sensitivity = c()
specificity = c()
RMSE = c()
Acc = c()
Sens = c()
Spec = c()
rmse = c()

for (i in 1:length(threshold)) {
  for (k in 1:10) {
    validation = training[((k-1)*round(0.1*dim(training)[1])+1):(k*round(0.1*dim(training)[1])-1),]
    train = training[-c(((k-1)*round(0.1*dim(training)[1])+1):(k*round(0.1*dim(training)[1])-1)),]
  
    mod2 = glm(osm_surf ~ rmean+gvar+bmean+bvar, data = train, family = binomial(link = logit))
  
    real.values = validation$osm_surf
  
    predicted.values = as.numeric(predict(mod2,newdata=validation,type="response")>threshold[i])
  
    confusion.matrix = table(real.values,predicted.values)
    confusion.matrix
    TP = confusion.matrix[2,2]
    TN = confusion.matrix[1,1]
    FP = confusion.matrix[1,2]
    FN = confusion.matrix[2,1]
    Acc = c(Acc,(TP+TN)/(TP+TN+FP+FN))
    Sens = c(Sens,TP/(TP+FN))
    Spec = c(Spec,TN/(TN+FP))
    rmse = c(rmse,sqrt(mean((real.values-predicted.values)^2)))
  
    remove(validation)
    remove(train)
  }

  accuracy = c(accuracy,mean(Acc))
  sensitivity = c(sensitivity,mean(Sens))
  specificity = c(specificity,mean(Spec))
  RMSE = c(RMSE,mean(rmse))
}

#Accuracy
accuracy
#Sensitivity
sensitivity
#Maximal sensitivity
max(sensitivity)
#Specificity
specificity

plot(threshold,accuracy,type = "l",xlab = "Thresholds",ylab = "Accuracy",
      lwd = 2,col = 'darkblue',ylim = c(0.5,1),xlim=c(0.1,0.6))
plot(threshold,sensitivity,type = "l",xlab = "Thresholds",ylab = 
       "Sensitivity",lwd = 2,col = 'darkblue',
       ylim = c(0.5,1),xlim=c(0.1,0.6))
points(threshold[match(max(sensitivity),sensitivity)],max(sensitivity),
       pch = 4,lwd = 3,cex = 1.5,col = 'blue')
plot(threshold,specificity,type = "l",xlab = "Thresholds",ylab = 
       "Specificity",lwd = 2,col = 'darkblue',
       ylim = c(0.5,1),xlim=c(0.1,0.6))
```
## Testing the model
Now that we have tried to come up with a good model, we can try to use it to make some prediction on the testing data set. The idea is to use the model to predict whether a street (for which we still know the condition of its surface) is 'paved' (1) or 'unpaved' (0) and confront the predicted values with the real values. Once we have obtained a good proportion of correct predictions, we can use the model on the rest of the streets for the paviment detection.
```{r}
#We predict the values on the data set 'testing' using the command #'predict()'
predicted.values = predict(mod2,newdata=testing,type="response")
#We set the cutoff to the value found during the goodness of fit and #diagnostic of the model
threshold = 0.35
predicted.values = ifelse(predicted.values>threshold,1,0)
head(predicted.values)
real.values = testing$osm_surf
#We build the confusion matrix
tab = table(real.values,predicted.values)
tab
#Accuracy
Accuracy = (tab[1,1]+tab[2,2])/(tab[1,1]+tab[1,2]+tab[2,1]+tab[2,2])
Accuracy
#Sensitivity
Sensitivity = tab[2,2]/(tab[2,1]+tab[2,2]) 
Sensitivity
#Specificity
Specificity = tab[1,1]/(tab[1,2]+tab[1,1])
Specificity
```
The conclusion is that we are able to predict correctly the 68.9% of the paved streets and the 86.7% of the unpaved ones. As far as we are concerned, we want the false negative to be as low as possible since it represents an additional cost to be work on more streets than the ones that actually need to be paved.

```{r}
id_misclassidied2 = c()
misclassified = c()

for (i in 1:dim(testing)[1]) {
  if (testing$osm_surf[i] != predicted.values[i]) {
    id_misclassidied2 = c(id_misclassidied2,testing$id[i])
    misclassified = c(misclassified,testing$osm_typo[i])
  }
}

id_misclassidied2

tab_misclassified <- factor(misclassified,levels = c('footway', 'primary', 'residentia', 'secondary', 'tertiary', 'unk'))
tab_misclassified = table(tab_misclassified)
tab_misclassified

tab_total = factor(testing$osm_typo,levels = c('footway', 'primary', 'residentia', 'secondary', 'tertiary', 'unk'))
tab_total = table(tab_total)
tab_total

percent = tab_misclassified/tab_total
percent
```
Looking at the tables below, we notice that the higher values of misclassified streets come from the types 'primary', 'tertiary, and, of course, 'unk'. It seems strange that all these errors in misclassifying the streets weight the same, so we think that adding 'osm_typo' as a predictor could be a way to solve this problem.

# Logistic regression on the streets of Maputo based on the distribution of the colors red, green and blue
Now we characterize the distribution of the colors red, green and blue using their minimum and maximum values and the median.
```{r}
mod3 = glm(osm_surf ~  
               rmean+rvar+rmed+rmin+rmax+gmean+gvar+gmed+gmin+gmax+bmean+bvar+
               bmed+bmin+bmax,data = training,family = binomial(link = logit))
summary(mod3)
BIC(mod3)
```
Looking at the p-values we can try to improve the model leaving behind less influential predictors.
```{r}
mod4 = glm(osm_surf ~  
               rmin+rmax+gmean+gvar+gmed+gmax+bvar+bmin,
               data = training,family = binomial(link = logit))
summary(mod4)
BIC(mod4)
```
We notice that both AIC and BIC have increased.

Let's do the Chi-squared test to see if one model is less informative than the other.
```{r}
anova(mod3,mod4,test = "Chisq")
```
The probability of the test statistic is large if the two models provided the same fit. In this case, we have to reject the null hypothesis (that the likelihoods of the two models are equivalent) since the p-value is smaller than 0.05 and this means that the model with less predictors is less informative than the more complex one. This is confirmed by looking at the AIC and BIC, since we know that the lower they are, the better the model is.

We tried to remove the statistically insignificant parameters and the AIC of the model did not improve: this migth be explained by the collinearity of the predictors.
```{r}
ggpairs(data = training, columns = 7:21, title ="Relationships between predictors", lower = list(continuous=wrap("points", alpha = 0.5, size=0.1)))
```
Or we can use the 'cor()' function.
```{r}
df = data.frame(data)
cor(df[,7:21])
```

When predictor variables in the same regression model are correlated, they cannot independently predict the value of the dependent variable. In other words, they explain some of the same variance in the dependent variable, which in turn reduces their statistical significance.

The potential solutions include the following:

* Remove some of the highly correlated independent variables.

* Linearly combine the independent variables, such as adding them together.

* Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression.

* LASSO and Ridge regression are advanced forms of regression analysis that can handle multicollinearity. If you know how to perform linear least squares regression, you’ll be able to handle these analyses with just a little additional study.

```{r}
#1
mod4 = glm(osm_surf ~  
               rmean+rmed+rmin+rmax+gmean+gvar+gmed+gmax+bvar+
               bmed+bmin+bmax,data = training,family = binomial(link = logit))
summary(mod4)
```

```{r}
#The good one
mod4 = glm(osm_surf ~  
               rmean+rmin+rmax+gmean+gvar+gmed+gmax+bvar+
               bmed+bmin,data = training,family = binomial(link = logit))
summary(mod4)
```

Now we need to find the optimal value for the threshold.
```{r}
threshold = c(0.2,0.25,0.3,0.35,0.4,0.45,0.5)
accuracy = c()
sensitivity = c()
specificity = c()
RMSE = c()
Acc = c()
Sens = c()
Spec = c()
rmse = c()

for (i in 1:length(threshold)) {
  for (k in 1:10) {
    validation = training[((k-1)*round(0.1*dim(training)[1])+1):(k*round(0.1*dim(training)[1])-1),]
    train = training[-c(((k-1)*round(0.1*dim(training)[1])+1):(k*round(0.1*dim(training)[1])-1)),]
  
    mod4 = glm(osm_surf ~  
               rmean+rmin+rmax+gmean+gvar+gmed+gmax+bvar+
               bmed+bmin,data = train,family = binomial(link = logit))
  
    real.values = validation$osm_surf
  
    predicted.values = as.numeric(predict(mod4,newdata=validation,type="response")>threshold[i])
  
    confusion.matrix = table(real.values,predicted.values)
    TP = confusion.matrix[2,2]
    TN = confusion.matrix[1,1]
    FP = confusion.matrix[1,2]
    FN = confusion.matrix[2,1]
    Acc = c(Acc,(TP+TN)/(TP+TN+FP+FN))
    Sens = c(Sens,TP/(TP+FN))
    Spec = c(Spec,TN/(TN+FP))
    rmse = c(rmse,sqrt(mean((real.values-predicted.values)^2)))
  
    remove(validation)
    remove(train)
  }

  accuracy = c(accuracy,mean(Acc))
  sensitivity = c(sensitivity,mean(Sens))
  specificity = c(specificity,mean(Spec))
  RMSE = c(RMSE,mean(rmse))
}

#Accuracy
accuracy
#Sensitivity
sensitivity
#Maximal sensitivity
max(sensitivity)
#Specificity
specificity

plot(threshold,accuracy,type = "l",xlab = "Thresholds",ylab = "Accuracy",
      lwd = 2,col = 'darkblue',ylim = c(0.5,1),xlim=c(0.1,0.6))
plot(threshold,sensitivity,type = "l",xlab = "Thresholds",ylab = 
       "Sensitivity",lwd = 2,col = 'darkblue',
       ylim = c(0.5,1),xlim=c(0.1,0.6))
points(threshold[match(max(sensitivity),sensitivity)],max(sensitivity),
       pch = 4,lwd = 3,cex = 1.5,col = 'blue')
plot(threshold,specificity,type = "l",xlab = "Thresholds",ylab = 
       "Specificity",lwd = 2,col = 'darkblue',
       ylim = c(0.5,1),xlim=c(0.1,0.6))
```
Confusion matrix.
```{r}
real.values = osm_surf

threshold = 0.35
mod4 = glm(osm_surf ~  
               rmean+rmin+rmax+gmean+gvar+gmed+gmax+bvar+
               bmed+bmin+bmax,data = training,family = binomial(link = logit))
predicted.values = as.numeric(mod4$fitted.values>threshold)

#Confusion matrix  
tab = table(real.values,predicted.values)
tab

#Accuracy
Accuracy = (tab[1,1]+tab[2,2])/(tab[1,1]+tab[1,2]+tab[2,1]+tab[2,2])
Accuracy
#Sensitivity
Sensitivity = tab[2,2]/(tab[2,1]+tab[2,2]) 
Sensitivity
#Specificity
Specificity = tab[1,1]/(tab[1,2]+tab[1,1])
Specificity
```

Now we test the model.
```{r}
predicted.values = predict(mod4,newdata=testing,type="response")
threshold = 0.35
predicted.values = ifelse(predicted.values>threshold,1,0)

real.values = testing$osm_surf

tab = table(real.values,predicted.values)
tab

#Accuracy
Accuracy = (tab[1,1]+tab[2,2])/(tab[1,1]+tab[1,2]+tab[2,1]+tab[2,2])
Accuracy
#Sensitivity
Sensitivity = tab[2,2]/(tab[2,1]+tab[2,2]) 
Sensitivity
#Specificity
Specificity = tab[1,1]/(tab[1,2]+tab[1,1])
Specificity
```
We want to see which typos of streets are misclassified the most.
```{r}
id_misclassidied4 = c()
misclassified = c()

for (i in 1:dim(testing)[1]) {
  if (testing$osm_surf[i] != predicted.values[i]) {
    id_misclassidied4 = c(id_misclassidied4,testing$id[i])
    misclassified = c(misclassified,testing$osm_typo[i])
  }
}

id_misclassidied4

tab_misclassified <- factor(misclassified,levels = c('footway', 'primary', 'residentia', 'secondary', 'tertiary', 'unk'))
tab_misclassified = table(tab_misclassified)
tab_misclassified

tab_total = factor(testing$osm_typo,levels = c('footway', 'primary', 'residentia', 'secondary', 'tertiary', 'unk'))
tab_total = table(tab_total)
tab_total

percent = tab_misclassified/tab_total
percent
```

# Adding 'osm_typo' as a predictor
```{r}
mod5 = glm(osm_surf ~  
               rmean+rmin+rmax+gmean+gvar+gmed+gmax+bvar+
               bmed+bmin+osm_typo,
               data = training,family = binomial(link = logit))
summary(mod5)
```
The problem is that 'osm_typo' is a categorical variable, so we need to change that defining the so called dummy variables.
```{r}
training$osm_typo_footway = ifelse(osm_typo == 'footway',1,0)
training$osm_typo_primary = ifelse(osm_typo == 'primary',1,0)
training$osm_typo_residentia = ifelse(osm_typo == 'residentia',1,0)
training$osm_typo_secondary = ifelse(osm_typo == 'secondary',1,0)
training$osm_typo_tertiary = ifelse(osm_typo == 'tertiary',1,0)
training$osm_typo_unk = ifelse(osm_typo == 'unk',1,0)
View(training)
```

```{r}
mod5 = glm(osm_surf ~  
               rmean+rmin+rmax+gmean+gvar+gmed+gmax+bvar+
               bmed+bmin+osm_typo_footway+osm_typo_primary+osm_typo_residentia+                osm_typo_secondary+osm_typo_tertiary+osm_typo_unk,
               data = training,family = binomial(link = logit))
summary(mod5)
```
The error message "1 not defined because of singularities" occurs when we fit the model and two or more predictor variables have an exact linear relationship between them (perfect multicollinearity). To fix this error, we can use the 'cor()' function to identify which variables in your dataset have a perfect correlation with each other and simply drop one of those variables from the regression model.
```{r}
df = data.frame(training)
cor(df[,c(7,10,11,12,13,14,16,18,19,20,23,24,25,26,27,28)])
```


```{r}
mod6 = glm(osm_surf ~  
               rmean+rmin+rmax+gmed+bvar+
               bmed+osm_typo_footway+osm_typo_residentia+                                     osm_typo_tertiary+osm_typo_unk,
               data = training,family = binomial(link = logit))
summary(mod6)

####
testing$osm_typo_footway = ifelse(testing$osm_typo == 'footway',1,0)
testing$osm_typo_primary = ifelse(testing$osm_typo == 'primary',1,0)
testing$osm_typo_residentia = ifelse(testing$osm_typo == 'residentia',1,0)
testing$osm_typo_secondary = ifelse(testing$osm_typo == 'secondary',1,0)
testing$osm_typo_tertiary = ifelse(testing$osm_typo == 'tertiary',1,0)
testing$osm_typo_unk = ifelse(testing$osm_typo == 'unk',1,0)
#View(testing)

predicted.values = predict(mod6,newdata=testing,type="response")
threshold = 0.35
predicted.values = ifelse(predicted.values>threshold,1,0)

real.values = testing$osm_surf

tab = table(real.values,predicted.values)
tab

#Accuracy
Accuracy = (tab[1,1]+tab[2,2])/(tab[1,1]+tab[1,2]+tab[2,1]+tab[2,2])
Accuracy
#Sensitivity
Sensitivity = tab[2,2]/(tab[2,1]+tab[2,2]) 
Sensitivity
#Specificity
Specificity = tab[1,1]/(tab[1,2]+tab[1,1])
Specificity

###
id_misclassidied6 = c()
misclassified = c()

for (i in 1:dim(testing)[1]) {
  if (testing$osm_surf[i] != predicted.values[i]) {
    id_misclassidied6 = c(id_misclassidied6,testing$id[i])
    misclassified = c(misclassified,testing$osm_typo[i])
  }
}

id_misclassidied6

tab_misclassified <- factor(misclassified,levels = c('footway', 'primary', 'residentia', 'secondary', 'tertiary', 'unk'))
tab_misclassified = table(tab_misclassified)
tab_misclassified

tab_total = factor(testing$osm_typo,levels = c('footway', 'primary', 'residentia', 'secondary', 'tertiary', 'unk'))
tab_total = table(tab_total)
tab_total

percent = tab_misclassified/tab_total
percent
```

# Comparison between the misclassified streets in the 3 models
We want to look for a link between the misclassified streets
```{r}
id_misclassidied2 #ids of the misclassified streets of model 2
id_misclassidied4 #ids of the misclassified streets of model 4
id_misclassidied6 #ids of the misclassified streets of model 6
```

## Application of the model on the unkown streets of Maputo
Finally, we use the model built in the previous sections to predict the surface for the remaining streets of the data set 'datonair' for which we do not know if they are paved or not.
```{r}
data = datonite[2559:5116,]
threshold = 0.2
predicted.values = ifelse(predict(mod.surf2,newdata=data,
                                  type="response")>threshold,1,0)
#Proportion of the paved streets predicted by our model
sum(predicted.values)/dim(data)[1]

for (i in 1:dim(data)[1]) {
  data$osm_surf[i]=predicted.values[i]
}
```
Great Maputo area and plot of the segments of the streets predicted through logistic regression.
```{r}
total = rbind(training,testing,data)

st_bbox(total)
(st_bbox(total)[3]-st_bbox(total)[1])*(st_bbox(total)[4]-st_bbox(total)[2])
sum(total$Length)*1e-03
i_asp = which(total$osm_surf==1)
i_unp = which(total$osm_surf==0)
ttt = character(dim(total)[1])
ttt[i_unp]="1. unpaved"; ttt[i_asp]="2. paved"
#windows();  
ggplot() + 
  geom_sf(data = total, aes(color=ttt,fill=ttt))+
  scale_fill_manual(values=c("#ffb01e", "grey45"))+
  scale_color_manual(values=c("#ffb01e", "grey45"))+
  labs(fill= "Pavement surface")+
  #ggtitle("Road network of the Greater Maputo area") + 
  coord_sf() +
  theme(panel.grid.major = element_line(color = gray(0.9), linetype=3, size=0.2), 
        panel.background = element_rect(fill="white"))+
  guides(color=FALSE)
```
And now we plot all the streets.
```{r}
data = st_read("Road_cleaned2.shp")
data = data[order(as.numeric(data$id)),]
total = total[order(as.numeric(total$id)),]
for (i in 1:dim(data)[1]) {
  stop = 1
  j = 1
  while (stop == 1 && j <= dim(total)[1]){
    if(data$id[i] == total$id[j]) {
      data$osm_surf[i] = total$osm_surf[j]
      stop = 0
    }
    else
      j = j+1
  }
}
# Greater Maputo area:
st_bbox(data) 
(st_bbox(data)[3]-st_bbox(data)[1])*(st_bbox(data)[4]-st_bbox(data)[2]) # area
sum(data$Length)*1e-03 # km
# plot di alcuni segmenti:
i_asp = which(data$osm_surf==1)
i_unp = which(data$osm_surf==0)
i_unk = which(data$osm_surf=="unk")
ttt = character(dim(data)[1])
ttt[i_unk]= "1. unknown"; ttt[i_unp]="2. unpaved"; ttt[i_asp]="3. paved"
#windows();  
ggplot() + 
  geom_sf(data = data, aes(color=ttt,fill=ttt))+
  scale_fill_manual(values=c("snow2", "#ffb01e", "grey45"))+
  scale_color_manual(values=c("snow2", "#ffb01e", "grey45"))+
  labs(fill= "Pavement surface")+
  #ggtitle("Road network of the Greater Maputo area") + 
  coord_sf() +
  theme(panel.grid.major = element_line(color = gray(.9), linetype=3, size=0.2), 
        panel.background = element_rect(fill="white"))+
  guides(color=FALSE)
```
